
import scipy.optimize as so
from sklearn.metrics import confusion_matrix
import math
import pandas as pd 
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import StandardScaler,normalize
from sklearn.metrics import classification_report, confusion_matrix 

from sklearn.metrics import accuracy_score 

data = pd.read_csv(r'C:\Users\abhis\Desktop\train - Copy.csv')
target_data = data['loan_default']
train = data.copy()


new = []

for i in train['AVERAGE.ACCT.AGE']:
    y = int(i[0])
    if i[6] == 'm':
        m = int(i[5])
    else:
        m = int(i[5:7])
    t = y*12 + m
    new.append(t)
train.loc[:,'AVERAGE.ACCT.AGE'] = new

new = []

for i in train['CREDIT.HISTORY.LENGTH']:
    y = int(i[0])
    if i[6] == 'm':
        m = int(i[5])
    else:
        m = int(i[5:7])
    t = y*12 + m
    new.append(t)
train.loc[:,'CREDIT.HISTORY.LENGTH'] = new
train['Date.of.Birth']  = pd.to_datetime(train['Date.of.Birth'])        
train['DisbursalDate']  = pd.to_datetime(train['DisbursalDate'])  
train['Employment.Type'] = train['Employment.Type'].astype('category')
train.head()
train = train.replace({'PERFORM_CNS.SCORE.DESCRIPTION':{'C-Very Low Risk':'Low', 'A-Very Low Risk':'Low',
                                                       'B-Very Low Risk':'Low', 'D-Very Low Risk':'Low',
                                                       'F-Low Risk':'Low', 'E-Low Risk':'Low', 'G-Low Risk':'Low',
                                                       'H-Medium Risk': 'Medium', 'I-Medium Risk': 'Medium',
                                                       'J-High Risk':'High', 'K-High Risk':'High','L-Very High Risk':'High',
                                                       'M-Very High Risk':'High','Not Scored: More than 50 active Accounts found':'Not Scored',
                                                       'Not Scored: Only a Guarantor':'Not Scored','Not Scored: Not Enough Info available on the customer':'Not Scored',
                                                        'Not Scored: No Activity seen on the customer (Inactive)':'Not Scored','Not Scored: No Updates available in last 36 months':'Not Scored',
                                                       'Not Scored: Sufficient History Not Available':'Not Scored', 'No Bureau History Available':'Not Scored'
                                                       }})                    




corr = train.corr().round(2)
f,ax = plt.subplots(figsize = (20,20))
#sns.heatmap(corr,annot=True)

col = list(corr.nlargest(11,'loan_default')['loan_default'].index)
corr['loan_default'].sort_values(ascending = False)
upper = corr.where(np.triu(np.ones(corr.shape),k=1).astype(np.bool))
to_drop = [col for col in upper.columns if any(upper[col]>=0.90)]




train = train.drop(['loan_default','PRI.DISBURSED.AMOUNT', 'SEC.SANCTIONED.AMOUNT', 'SEC.DISBURSED.AMOUNT','UniqueID'],axis='columns')

train['Employment.Type'] = train['Employment.Type'].fillna(method = 'bfill')

num_data = train.select_dtypes(include=['int64','float64'])
scaler = StandardScaler()
scaled_data = pd.DataFrame(scaler.fit_transform(num_data),columns=num_data.columns)
normalized_data = pd.DataFrame(normalize(num_data),columns=num_data.columns)
cat_data = train.select_dtypes(include=['object'])


cat_data = pd.get_dummies(cat_data)
#cat_data.head()

train_final = pd.concat([normalized_data,cat_data],axis='columns')
train_final =train.copy()
#train_final.hist(figsize=(20,20))


train_final = train_final.replace({'Employment.Type':{'Salaried':1, 'Self employed':0}})                                   
#print(train_final.dtypes)
train_final = train_final.replace({'PERFORM_CNS.SCORE.DESCRIPTION':{'Not Scored':0, 'Low':1,'Medium':2, 'High':3 }}) 
                                                                             
train_final = train_final.select_dtypes(include=['int64','float64'])                                                      
                                                                     
#print(train_final.dtypes)
from sklearn.model_selection import train_test_split  
X_train, X_test, y_train, y_test = train_test_split(train_final, target_data, test_size = 0.20)  

X = np.c_[np.ones((X_train.shape[0], 1)), X_train]
X_test = np.c_[np.ones((X_test.shape[0], 1)), X_test]
y = y_train[:, np.newaxis]
theta =np.zeros((X.shape[1], 1))
#print(X.shape)
#print(theta.shape)
def sigmoid(X,theta):
    z = np.dot(X,theta)
    return 1.0/(1+np.exp(-z))


def cost_function(h,y):
    loss = ((-y * np.log(h))-((1-y)* np.log(1-h))).mean()
    return loss
#print(cost_function(h,y))

def gradient_descent(X,h,y):
    return np.dot(X.T,(h-y))/y.shape[0]

#gradient = gradient_descent(X,h,y)


def update_loss(theta,learning_rate,gradient):
    return theta-(learning_rate*gradient)




#print(update_loss(theta,0.1,gradient))
def predict(X,theta):
    threshold = 0.5
    outcome = []
    result = sigmoid(X,theta)
    
    for i in range(X.shape[0]):
        if result[i] <= threshold:
            outcome.append(0)
        else:
            outcome.append(1)
    return outcome



cost = []
#print(predict(X,theta))

cost = []
#print(theta)
for i in range(0,1000):
        h = sigmoid(X,theta)
        cost.append(cost_function(h,y))
        gradient = gradient_descent(X,h,y)
        theta = update_loss(theta,0.1,gradient)
        
 
 

outcome = predict(X_test,theta)

#print(outcome)


metric = confusion_matrix(y_test,outcome)
print("Confusion Matrix for Logistic Regression Model is")  
print(metric)
acc = accuracy_score(y_test, outcome) 

print("Classification Report for Logistic Regression Model is")
print(classification_report(y_test, outcome)) 
    
print("Accuracy for Logistic Regression Model is")    
print(acc)    
    




















