import numpy as np
from sklearn.preprocessing import StandardScaler,normalize

from sklearn.metrics import accuracy_score 
import pandas as pd
data = pd.read_csv(r'C:\Users\abhis\Desktop\train - Copy.csv')
target_data = data['loan_default']
train = data.copy()


new = []

for i in train['AVERAGE.ACCT.AGE']:
    y = int(i[0])
    if i[6] == 'm':
        m = int(i[5])
    else:
        m = int(i[5:7])
    t = y*12 + m
    new.append(t)
train.loc[:,'AVERAGE.ACCT.AGE'] = new

new = []

for i in train['CREDIT.HISTORY.LENGTH']:
    y = int(i[0])
    if i[6] == 'm':
        m = int(i[5])
    else:
        m = int(i[5:7])
    t = y*12 + m
    new.append(t)
train.loc[:,'CREDIT.HISTORY.LENGTH'] = new
train['Date.of.Birth']  = pd.to_datetime(train['Date.of.Birth'])        
train['DisbursalDate']  = pd.to_datetime(train['DisbursalDate'])  
train['Employment.Type'] = train['Employment.Type'].astype('category')
train.head()
train = train.replace({'PERFORM_CNS.SCORE.DESCRIPTION':{'C-Very Low Risk':'Low', 'A-Very Low Risk':'Low',
                                                       'B-Very Low Risk':'Low', 'D-Very Low Risk':'Low',
                                                       'F-Low Risk':'Low', 'E-Low Risk':'Low', 'G-Low Risk':'Low',
                                                       'H-Medium Risk': 'Medium', 'I-Medium Risk': 'Medium',
                                                       'J-High Risk':'High', 'K-High Risk':'High','L-Very High Risk':'High',
                                                       'M-Very High Risk':'High','Not Scored: More than 50 active Accounts found':'Not Scored',
                                                       'Not Scored: Only a Guarantor':'Not Scored','Not Scored: Not Enough Info available on the customer':'Not Scored',
                                                        'Not Scored: No Activity seen on the customer (Inactive)':'Not Scored','Not Scored: No Updates available in last 36 months':'Not Scored',
                                                       'Not Scored: Sufficient History Not Available':'Not Scored', 'No Bureau History Available':'Not Scored'
                                                       }})                    




corr = train.corr().round(2)
#f,ax = plt.subplots(figsize = (20,20))
#sns.heatmap(corr,annot=True)

col = list(corr.nlargest(11,'loan_default')['loan_default'].index)
corr['loan_default'].sort_values(ascending = False)
upper = corr.where(np.triu(np.ones(corr.shape),k=1).astype(np.bool))
to_drop = [col for col in upper.columns if any(upper[col]>=0.90)]




train = train.drop(['PRI.DISBURSED.AMOUNT', 'SEC.SANCTIONED.AMOUNT', 'SEC.DISBURSED.AMOUNT','UniqueID'],axis='columns')

train['Employment.Type'] = train['Employment.Type'].fillna(method = 'bfill')

num_data = train.select_dtypes(include=['int64','float64'])
scaler = StandardScaler()
scaled_data = pd.DataFrame(scaler.fit_transform(num_data),columns=num_data.columns)
normalized_data = pd.DataFrame(normalize(num_data),columns=num_data.columns)
cat_data = train.select_dtypes(include=['object'])

cat_data = pd.get_dummies(cat_data)
#cat_data.head()

train_final = pd.concat([normalized_data,cat_data],axis='columns')
train_final =train.copy()
#train_final.hist(figsize=(20,20))


train_final = train_final.replace({'Employment.Type':{'Salaried':1, 'Self employed':0}})                                   
#print(train_final.dtypes)
train_final = train_final.replace({'PERFORM_CNS.SCORE.DESCRIPTION':{'Not Scored':0, 'Low':1,'Medium':2, 'High':3 }}) 
                                                                             
train_final = train_final.select_dtypes(include=['int64','float64'])


def check_purity(data):
    label_column = data[:,-1]    
    unique_class = np.unique(label_column)
    
    if len(unique_class) == 1:
        return True
    else :
        return False

#print(check_purity(train_final.values))
    
    
def classify_data(data):
    label_column = data[:,-1] 
    unique_class,count_unique_class = np.unique(label_column,return_counts = True)
    index = count_unique_class.argmax()
    classification = unique_class[index]
    return classification


#print(classify_data(train_final.values))


def get_potential_splits(data):
    potential_splits = {}
    row,column = data.shape
    for column_index in range(column - 1):
        potential_splits[column_index] = []
        values = data[:,column_index]
        unique_values = np.unique(values)
         
        for x in range(len(unique_values)):
            if x != 0:
                current = unique_values[x]
                previous = unique_values[x-1]
                P_C = (current + previous)/2
                   
                potential_splits[column_index].append(P_C)
    
    return potential_splits
    

    
Man = get_potential_splits(train_final.values)
#print(Man)    
def split_data(data,s_c,s_v) :  
    s_c_values = data[:,s_c]
    d_b = data[s_c_values <= s_v]
    d_a = data[s_c_values > s_v]
    
    
    return d_b, d_a

def Entropy(data):
    label_column = data[:,-1] 
    unique_class,count_unique_class = np.unique(label_column,return_counts = True)
    prob = count_unique_class/count_unique_class.sum()
    entropy = sum(prob * -np.log2(prob))
    return entropy

#print( Entropy(train_final.values))
def c_o_e(d_b,d_a) :
    n = len(d_b) + len(d_a)
    p_d_b = len(d_b)/n
    p_d_a = len(d_a)/n
    o_e = (p_d_b * Entropy(d_b) + p_d_a * Entropy(d_a))
    return o_e
 
def d_b_s ( data,p_s):
    o_e = 999
    for column_index in p_s:
        for value in p_s[column_index]:
            d_b,d_a = split_data(data,s_c = column_index,s_v = value)
            current_O_entropy = c_o_e(d_b,d_a)
            if current_O_entropy < o_e:
                o_e =  current_O_entropy
                b_s_c = column_index
                b_s_v = value
                return b_s_c,b_s_v

s_c,s_v = d_b_s (train_final.values,Man)                
Data_b,Data_a = split_data(train_final.values,s_c,s_v)              
#print(Data_b)                
def Algorithm(train_final,c = 0,min_s = 2,max_depth = 6):                
    
     
        
    if c == 0:
        
        data = train_final.values
        
    else:
        data =  train_final  
    if (check_purity(data)) or (len(data) < min_s) or (c == max_depth):
        classification = classify_data(data)
        return classification
   
    else:
        c = c + 1
        
        potential_splits =   get_potential_splits(data)
        split_column,split_value = d_b_s ( data,potential_splits)
        d_b,d_a = split_data(data,split_column,split_value)
        
        yes = Algorithm(d_b,c,min_s,max_depth)
        No = Algorithm(d_a,c,min_s,max_depth)

        Ques = " {} <= {} ".format(split_column,split_value)
        sub = {Ques:[]}
        
        if yes == No:
            sub = yes
        else :    
            sub[Ques].append(yes)
            sub[Ques].append(No)
            return sub

    

def Classify(X,tree) :
    Ques = list(tree.keys())[0]
    feature,op,value = Ques.split()
    if X[feature] <= float(value):
        ans = tree[Ques][0]
    else :
        ans = tree[Ques][1]
        
    if not isinstance (ans,dict):
        return ans
    else :
        r_tree = ans
        return Classify(X,r_tree)
    
    
target_data = train_final['loan_default']
train_final_1 = train_final.drop(['loan_default'], axis=1)    
from sklearn.model_selection import train_test_split
xTrain, xTest, yTrain, yTest = train_test_split(train_final_1, target_data, test_size = 0.2, random_state = 0)

def accuracy(data,tree):
    data["c"] = data.apply(Classify,axis = 1,args = (tree,))
    data["c_corr"] = bool(data["c"]) == bool(data['loan_default'])
    acc = data.c_corr.mean()
    return acc
tree = Algorithm(train_final)
Yup = accuracy(train_final,tree)
print(Yup)